<!doctype html>
<html lang="en">
  <head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <!-- load bootstrap -->
  <link rel="stylesheet" href="/assets/css/bootstrap.min.css">
  <script src="/assets/js/bootstrap.bundle.min.js"></script>

  <!-- user -->
  
    <link rel="shortcut icon" type="image/png" href="/assets/images/SparklingCorrelation-icon.png">
  
  <link rel="stylesheet" href="/assets/css/open-color.css">
  <link rel="stylesheet" href="/assets/css/styles.css">
  <script src="/assets/js/main.js"></script>

  <!-- font awesome -->
  <script async src="https://use.fontawesome.com/releases/v5.0.12/js/all.js"></script>

  <link type="application/atom+xml" rel="alternate" href="https://sparklingcorrelation.com/feed.xml" title="Sparkling Correlation" />
  <!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Build a Neural Network with Julia | Sparkling Correlation</title>
<meta name="generator" content="Jekyll v3.9.0" />
<meta property="og:title" content="Build a Neural Network with Julia" />
<meta name="author" content="Stephen Lee" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Audience If you’re reading this, I assume you are already motivated to learn how a neural network actually works. They are relevant and powerful tools, so the sell shouldn’t be too difficult anyway. There are many other excellent guides out there that I will link to below. If you find any mistakes or have comments, please email me or comment on this post." />
<meta property="og:description" content="Audience If you’re reading this, I assume you are already motivated to learn how a neural network actually works. They are relevant and powerful tools, so the sell shouldn’t be too difficult anyway. There are many other excellent guides out there that I will link to below. If you find any mistakes or have comments, please email me or comment on this post." />
<link rel="canonical" href="https://sparklingcorrelation.com/machine%20learning/prediction/tutorial/2020/12/20/build-a-neural-network.html" />
<meta property="og:url" content="https://sparklingcorrelation.com/machine%20learning/prediction/tutorial/2020/12/20/build-a-neural-network.html" />
<meta property="og:site_name" content="Sparkling Correlation" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-12-20T00:00:00-06:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Build a Neural Network with Julia" />
<script type="application/ld+json">
{"author":{"@type":"Person","name":"Stephen Lee"},"url":"https://sparklingcorrelation.com/machine%20learning/prediction/tutorial/2020/12/20/build-a-neural-network.html","headline":"Build a Neural Network with Julia","dateModified":"2020-12-20T00:00:00-06:00","datePublished":"2020-12-20T00:00:00-06:00","description":"Audience If you’re reading this, I assume you are already motivated to learn how a neural network actually works. They are relevant and powerful tools, so the sell shouldn’t be too difficult anyway. There are many other excellent guides out there that I will link to below. If you find any mistakes or have comments, please email me or comment on this post.","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://sparklingcorrelation.com/machine%20learning/prediction/tutorial/2020/12/20/build-a-neural-network.html"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->


  

</head>

  <body>
    <div id="main" class="container">

      <nav class="navbar navbar-expand-lg fixed-top navbar-light py-4" id="menu">

  <!-- BRAND LOGO OR NAME -->
  <a class="navbar-brand top" id="site-logo" href="/">
    
      <img class="navbar-brand-img" src="/assets/images/SparklingCorrelation.png" alt="">
    
  </a>
  
  <!-- NAVIGATION -->
  
    
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>
    
    <div class="collapse navbar-collapse justify-content-end" id="navbarCollapse">
      <div class="navbar-nav">
      
        
        <a class="nav-item nav-link " href="/index.html">Home</a>
      
        
        <a class="nav-item nav-link " href="/categories.html">Categories</a>
      
        
        <a class="nav-item nav-link " href="/archive.html">Archive</a>
      
      </div>
    </div>
  
</nav>


      <main>
        <!-- jquery -->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>

<script src="/assets/js/adjustAnchorLink.js"></script>
<script src="/assets/js/liveTOCScroll.js"></script>

<div class="post-body">
  <div class="row post-header">
    <div class="col">
      <h1 class="post-title text-center">Build a Neural Network with Julia</h1>
      <div class="post-meta text-center">

        
          
            <span class="post-metadata" itemprop="author" itemscope itemtype="http://schema.org/Person">
              Stephen Lee -
            </span>
            
          
        

        
        <time class="post-metadata" datetime="2020-12-20T00:00:00-06:00" itemprop="datePublished">
          Dec 20, 2020
        </time>
        
      </div>
    </div>
  </div>

  <div class="row">

    <!-- START COL WITH TOC -->
    <div class="col-sm-2 pt-3">
      <div class="toc-div py-3">
  
    <h3 class="toc-title">Table of Contents</h3>
  
  <ul class="toc"><li><a href="#audience">Audience</a></li><li><a href="#introduction">Introduction</a></li><li><a href="#components-and-definitions">Components and Definitions</a></li><li><a href="#big-picture">Big Picture</a><ul><li><a href="#network">Network</a></li><li><a href="#activation">Activation</a></li><li><a href="#loss">Loss</a></li><li><a href="#backpropagation">Backpropagation</a></li><li><a href="#stochastic-gradient-descent">Stochastic Gradient Descent</a></li></ul></li><li><a href="#implementation">Implementation</a><ul><li><a href="#network-1">Network</a></li><li><a href="#layer">Layer</a><ul><li><a href="#initializing-the-layer">Initializing the Layer</a></li></ul></li><li><a href="#feedforward">Feedforward</a></li><li><a href="#backpropagation-1">Backpropagation</a></li><li><a href="#stochastic-gradient-descent-1">Stochastic Gradient Descent</a></li></ul></li><li><a href="#conclusion">Conclusion</a></li><li><a href="#resources">Resources</a></li><li><a href="#footnotes">Footnotes</a></li></ul>

</div>

    </div>

    <!-- MAIN POST -->
    <div class="col-sm-10 post p-4">
        <h2 id="audience">Audience</h2>
<p>If you’re reading this, I assume you are already motivated to learn how a neural network actually works. They are relevant and powerful tools, so the sell shouldn’t be too difficult anyway. There are many other excellent guides out there that I will link to below. If you find any mistakes or have comments, please email me or comment on this post.</p>

<h2 id="introduction">Introduction</h2>
<p>I implement a feedforward, fully-connected neural network<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote">1</a></sup> with stochastic gradient descent in the Julia programming language (terms are defined below). There are two main advantages of this setup:</p>

<ol>
  <li>
    <p>This architecture represents a neural network in its simplest <em>feasible</em> form.<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote">2</a></sup> Many other implementations, like convolutional or recurrant neural networks, are extensions of this base framework.</p>
  </li>
  <li>
    <p>Julia is a fantastic language for learning this since it doesn’t distract from the math (i.e. matrix math is built in with convenient syntax), but still allows for powerful programming features that let us abstract from certain implementation details (i.e. using structs to create our own objects).</p>
  </li>
</ol>

<p>Note, this is for educational purposes only: it is (hopefully) optimized for readability rather than speed or scalability. For a production grade neural network in Julia, check out Flux.jl.</p>

<p>The full code is available here:</p>
<ul>
  <li><a href="https://github.com/slee981/neural-network">https://github.com/slee981/neural-network</a>.</li>
</ul>

<h2 id="components-and-definitions">Components and Definitions</h2>
<ul>
  <li><strong>Neural Network</strong>: A sequence of mathematical operations, philosophically inspired by a model of the brain, and designed to iteratively improve its fit of historical data, while maintaining the ability to generalize to new, unseen data.</li>
  <li><strong>Layer</strong>: A building block of a neural network that receives input, performs a basic linear (technically, affine) transformation, and passes the affine transformation through a non-linear “activation function” as its output.</li>
  <li><strong>Feedforward</strong>: The forward process of receiving an input, and progressing through the network layer by layer until you reach the final output.</li>
  <li><strong>Fully-connected</strong>: A type of neural network where each layer’s inputs are “connected” to the final output via its own potentially unique relationship i.e. no inputs are ignored, and no weights are systematically repeated.</li>
  <li><strong>Affine Transformation</strong>: A geometric transformation that preserves lines and parallelism, but not necessarily distances and angles [source: Wikipedia]. For our use case, it is simply a transformation of the form \(z = Ax + b\). This technically differs from a linear transformation, since a linear transformation doesn’t have a “bias” component.</li>
  <li><strong>Activation Function</strong>: A non-linear function applied element wise to a vector. Two common examples are the sigmoid and relu functions.</li>
  <li><strong>Loss Function</strong>: A function that allows us to measure how well a predicted value matches the known “true” value. This is equivalently called a cost function.</li>
  <li><strong>Stochastic Gradient Descent</strong>: An optimization process by which training a neural network is made feasible. Rather than calculating the true loss over the entire dataset before making a single update to our parameters, we only calculate the loss on a random subsample of the data and then update our weights accordingly. Emperically, this process is found to produce more general results as well as speed up the training process.</li>
</ul>

<h2 id="big-picture">Big Picture</h2>
<p>The goal of a neural network is to approximate a function, \(f(x)\), such that it is wrong as little as possible when it sees new data. We start with a training dataset containing \(N\) observations of \(k\) features (aka covariates), as well as \(N\) observations of some outcome that we will try to predict. For this exercise, I will ignore any use of a test dataset to focus on the math of the neural network.</p>

<p>We can conveniently store the features in an \(N\times k\) matrix called \(X\), and similarly store the outcomes in a matrix \(Y\). Thus, a length-\(k\) horizontal row vector \(x_i\) describes the features for observation \(i\) (for example, this could be a specific house, person, photo, or document depending on your data).</p>

<p>Visually, we can imagine this as follows:</p>

\[%katex
\begin{aligned}
f\Bigg(\begin{bmatrix}
x_{11} &amp; \ldots &amp; x_{1k}  \\
\vdots &amp; \ddots &amp; \vdots \\ 
x_{N1} &amp; \ldots &amp; x_{Nk} 
\end{bmatrix}  \Bigg) &amp;\approx \begin{bmatrix}
y_1 \\
y_2 \\
\vdots \\ 
y_N
\end{bmatrix} \\
f\Bigg(\begin{bmatrix}
x_{1}^T  \\
\vdots  \\ 
x_{N}^T  
\end{bmatrix}  \Bigg) &amp;\approx 
\end{aligned}\]

<h3 id="network">Network</h3>
<p>Together, the neural network acts to:</p>
<ol>
  <li>Receive an observation’s input, \(x_i\). This is the same as an individual row vector above, just transposed as a \(k \times 1\) column vector.</li>
  <li>Take a linear (technically, affine) transformation of the input features, i.e. \(Ax_i + b\). Here, the matrix \(A\) has dimensions \(p \times k\), and the “bias” \(b\) is, accordingly, a \(p \times 1\) column vector.</li>
  <li>Pass the resulting vector into a “non-linear activation function”, i.e. \(w_l = \sigma(Ax_i + b)\). The output here will be a \(p \times 1\) column vector.</li>
  <li>Use this output vector, \(w_l\), as the input to the next layer and repeat the process until you reach the last layer.</li>
  <li>Compare the last predicted output \(w_L\) to the observed target output \(y_i\) (using a loss function).</li>
  <li>Calculate the gradient of the loss function with respect to the weights you can control in each layer, i.e. all \(A\)’s and \(b\)’s.</li>
  <li>Update the weights in the direction of maximum change i.e. the <em>negative</em> gradient. This is what the commonly used names backpropagation and gradient descent refer to.</li>
  <li>Repeat until satisfied.</li>
</ol>

<p>Visually, we can represent the network as follows.</p>

<p><img src="/assets/images/NeuralNet.png" alt="Neural Network" /></p>

<p>Note, this diagram is a bit different than the diagrams often shown. For me, this is more useful as it helps to internalize the fact that each layer is only performing basic math operations during the forward steps - no magic. Again, each layer will receive a vector of input, calculate a linear transformation (which returns another vector of possibly different length), pass each element of the new vector into a non-linear “activation” function, and then output that result to the next layer until there are no more layers.</p>

<h3 id="activation">Activation</h3>
<p>We apply a non-linear “activation” function to the result of the linear transformation in order to allow the neural network to capture non-linear relationships. Conceptually, it is that simple.</p>

<p>Two very common activation functions are sigmoid and relu, defined as follows:</p>

\[%katex
\begin{aligned}
\sigma_{sigmoid}(z) &amp;= \frac{1}{1 + e^{-z}} \\ \\
\sigma_{relu}(z) &amp;= max(0, z)
\end{aligned}\]

<p>In this case, the activation function is applied element-wise to the linear transformation step, i.e.:</p>

\[%katex
\begin{aligned}
w_l &amp;= \sigma(Ax + b) \\ 
&amp;= \sigma\Bigg( \begin{bmatrix}
a_{11} &amp; \ldots &amp; a_{1N}  \\
\vdots &amp; \ddots &amp; \vdots \\ 
x_{m1} &amp; \ldots &amp; a_{1N} 
\end{bmatrix} 
\begin{bmatrix}
x_1 \\
x_2 \\
\vdots \\ 
x_N
\end{bmatrix} + 
\begin{bmatrix}
b_1 \\
\vdots \\ 
b_m
\end{bmatrix}
\Bigg) \\
&amp;= \sigma\Bigg( \begin{bmatrix}
z_1 \\
\vdots \\ 
z_m
\end{bmatrix} \Bigg) \\
&amp;= \begin{bmatrix}
w_1 \\
\vdots \\ 
w_m
\end{bmatrix}
\end{aligned}\]

<p>Without the activation function, \(\sigma\), the resulting series of linear tranasformations could be simplified to just a single simple linear transformation: we would never be able to fit more complex relationships. To convince youself this is true, consider the following example of a hypothetical three (3) layer network without an activation function:</p>

\[%katex
\begin{aligned} 
\underbrace{A_3(\underbrace{A_2(\underbrace{A_1x_1 + b_1}_{\text{Layer 1}}) + b_2)}_{\text{Layer 2}} + b3}_{\text{Layer 3}} &amp;= A_3 A_2 A_1 x_1 + A_3 A_2 b_1 + A_3 b_2 + b_3 \\ 
&amp;= A_sx_1 + b_s
\end{aligned}\]

<p>Where we let \(A_s = A_3 A_2 A_1\) and \(b_s = A_3 A_2 b_1 + A_3 b_2 + b_3\). This simplification will hopefully convince you that an \(L\) layer neural network without a non-linear activation function applied to each of the layers will ultimately reduce to the mathematical equilivant of a single linear transformation.</p>

<p>If you need additional convincing, this Julia code will reproduce the above relationship:</p>
<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x1</span> <span class="o">=</span> <span class="n">randn</span><span class="x">(</span><span class="mi">3</span><span class="x">,</span> <span class="mi">1</span><span class="x">)</span>

<span class="n">A1</span> <span class="o">=</span> <span class="n">randn</span><span class="x">(</span><span class="mi">4</span><span class="x">,</span> <span class="mi">3</span><span class="x">)</span>
<span class="n">A2</span> <span class="o">=</span> <span class="n">randn</span><span class="x">(</span><span class="mi">6</span><span class="x">,</span> <span class="mi">4</span><span class="x">)</span>
<span class="n">A3</span> <span class="o">=</span> <span class="n">randn</span><span class="x">(</span><span class="mi">2</span><span class="x">,</span> <span class="mi">6</span><span class="x">)</span>

<span class="n">b1</span> <span class="o">=</span> <span class="n">randn</span><span class="x">(</span><span class="mi">4</span><span class="x">,</span> <span class="mi">1</span><span class="x">)</span>
<span class="n">b2</span> <span class="o">=</span> <span class="n">randn</span><span class="x">(</span><span class="mi">6</span><span class="x">,</span> <span class="mi">1</span><span class="x">)</span>
<span class="n">b3</span> <span class="o">=</span> <span class="n">randn</span><span class="x">(</span><span class="mi">2</span><span class="x">,</span> <span class="mi">1</span><span class="x">)</span>

<span class="n">As</span> <span class="o">=</span> <span class="n">A3</span><span class="o">*</span><span class="n">A2</span><span class="o">*</span><span class="n">A1</span>
<span class="n">bs</span> <span class="o">=</span> <span class="n">A3</span><span class="o">*</span><span class="n">A2</span><span class="o">*</span><span class="n">b1</span> <span class="o">+</span> <span class="n">A3</span><span class="o">*</span><span class="n">b2</span> <span class="o">+</span> <span class="n">b3</span>

<span class="n">full</span>       <span class="o">=</span> <span class="n">A3</span><span class="o">*</span><span class="x">(</span><span class="n">A2</span><span class="o">*</span><span class="x">(</span><span class="n">A1</span><span class="o">*</span><span class="n">x1</span> <span class="o">+</span> <span class="n">b1</span><span class="x">)</span> <span class="o">+</span> <span class="n">b2</span><span class="x">)</span> <span class="o">+</span> <span class="n">b3</span>
<span class="n">simplified</span> <span class="o">=</span> <span class="n">As</span><span class="o">*</span><span class="n">x1</span> <span class="o">+</span> <span class="n">bs</span>

<span class="n">isapprox</span><span class="x">(</span><span class="n">full</span><span class="x">,</span> <span class="n">simplified</span><span class="x">)</span>
<span class="c"># true</span>
<span class="c">#</span>
<span class="c"># note: becuase of floating point precision</span>
<span class="c"># the answers may be slightly different in the smallest decimal places</span>
<span class="c"># which is why I use `isapprox`.</span>
</code></pre></div></div>

<h3 id="loss">Loss</h3>
<p>How do we evaluate success? Our neural network produces <em>some</em> output, and we may want to know how good that output is. You will hear this concept referred to as, equivalently, loss, cost, or objective functions.</p>

<p>The idea is simple: provide a way to quantify how good our model fits the data. In practice, this involves penalizing deviations from the true labeled results. If our model guesses the answer should be 0.21 when we know the answer is 1, we want the loss (cost) to be larger than if it had guessed 0.85.</p>

<p>In this write-up, I use the mean-squared-error loss function (largely due to the symmetry with standard OLS regression). There are other resources with great primers on other common loss functions, so I won’t cover it here. The important part is that we want to minimize the loss - and to do that we use calculus.</p>

<h3 id="backpropagation">Backpropagation</h3>
<p>Once we have made a guess at what the answer should be, \(w_L\), we compare it to the target output \(y_i\) and realize it may not be exactly correct. In this case our cost (loss) function \(C\) will produce a larger value than we want. How do we adjust the numbers in each layer’s weight matrix \(A_l\) and “bias”<sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote">3</a></sup> vector \(b_l\), such that the value of loss function will decrease i.e. our predicted output will be closer to the target output? We take derivatives.</p>

<p>In math, what we want to know is this: what is the partial derivative of the loss function with respect to each of the numbers I can control i.e. the elements of each weight matrix \(A_l\) and the bias vectors \(b_l\). Put another way, we want to find:</p>

\[%katex
\frac{\partial C}{\partial A_l} \, , \, \frac{\partial C}{\partial b_l}\]

<p>For each layer \(l \in [1, L]\). Important note: I will take some liberties with notation here to avoid additional superscripts and/or subscripts. Technically, a partial derivative must be taken with respect to a specific element, for example, \(\frac{\partial C}{\partial a_{ij}^l}\) for  \(a_{ij}^l \in A_l\). In this case, I use notation that represents a “partial derivative of the cost function with respect to each of the elements of \(A_l\) and \(b_l\) in a layer”. Thus, the result of these family of derivatives I show will be vectors and matrices, rather than scalars.</p>

<p>To do so, we use a technique commonly called “backpropagation”, and also known as “reverse mode differentiation”.<sup id="fnref:4" role="doc-noteref"><a href="#fn:4" class="footnote">4</a></sup> In short, rather than start from the input value and start chaining deravitives until you get to the output, we start with the output and work backwards to the input. This way, in one pass, we find all the relevant deravitives we care about. Check out the footnote in this paragraph for an excellent introduction to the concept.</p>

<p>Thus, starting with the last layer, we find deravitives:</p>

\[%katex
\begin{aligned}
\frac{\partial C}{\partial A_L} &amp;= \textcolor{blue}{\frac{\partial C}{\partial w_L}} \textcolor{red}{\frac{\partial w_L}{\partial z_L}} \textcolor{goldenrod}{\frac{\partial z_L}{\partial A_{L}}} \\ \\
\frac{\partial C}{\partial b_L} &amp;= \textcolor{blue}{\frac{\partial C}{\partial w_L}} \textcolor{red}{\frac{\partial w_L}{\partial z_L}} \textcolor{seagreen}{\frac{\partial z_L}{\partial b_{L}}} 
\end{aligned}\]

<p>It’s important to remember that we pick the relevant functions. So, for example, if we use a variant of the mean square error cost (loss) function we find</p>

\[%katex
\begin{aligned}
C &amp;= \frac{1}{2}\sum_i (w_{Li} - y_i)^2  &amp; \Leftarrow \text{Cost function} \\ 
\textcolor{blue}{\frac{\partial C}{\partial w_{Li}}} &amp;= \textcolor{blue}{w_{Li} - y_i} &amp; \Leftarrow \text{Vector}
\end{aligned}\]

<p>Note that when our target \(y_i\) is a vector instead of a scalar (e.g. for a categorical problem), we often choose a method of aggregating the losses for each element of that vector. In the Google framework Tensorflow, for example, they call this a reduction.<sup id="fnref:5" role="doc-noteref"><a href="#fn:5" class="footnote">5</a></sup> The main reason for this is that it simplifies the math and calculations significantly. This simplification only applies to the reported “cost” (loss) value itself, not the derivative, however, which can remain a vector.</p>

<p>We can calculate the other necessary partial derivatives as well.</p>

\[%katex
\begin{aligned}
w_L &amp;= \sigma\big(\underbrace{A_L w_{L-1} + b_L}_{z_L} \big) \\
&amp;= \sigma(z_L ) \\ \\
\textcolor{red}{\frac{\partial w_L}{\partial z_{L}}} &amp;= \textcolor{red}{\sigma'(z_L)} &amp; \Leftarrow \text{Matrix} \\
\textcolor{goldenrod}{\frac{\partial z_L}{\partial A_{L}}} &amp;= \textcolor{goldenrod}{w_{L-1}} &amp; \Leftarrow \text{Vector} \\
\textcolor{seagreen}{\frac{\partial z_L}{\partial b_{L}}} &amp;= \textcolor{seagreen}{1}
\end{aligned}\]

<p>We can now put these values together to find our first relevant gradients (rearranging to fit the necessary geometry for matrix algebra):</p>

\[%katex
\begin{aligned}
\frac{\partial C}{\partial A_L} &amp;= \textcolor{red}{\sigma'(z_L)} \textcolor{blue}{\bigg(w_{Li} - y_i\bigg)} \textcolor{goldenrod}{(w_{L-1})^T} \\ \\
\frac{\partial C}{\partial b_L} &amp;= \textcolor{red}{\sigma'(z_L)} \textcolor{blue}{\bigg(w_{Li} - y_i\bigg)} \textcolor{seagreen}{(1)} 
\end{aligned}\]

<p>More generally, using the chain rule, we can calculate the gradient for any layer as follows:</p>

\[%katex
\begin{aligned}
\frac{\partial C}{\partial A_l} &amp;= \textcolor{blue}{\frac{\partial C}{\partial w_L}} \textcolor{red}{\frac{\partial w_L}{\partial z_L}} \textcolor{violet}{\frac{\partial z_L}{\partial w_{L-1}}} \ldots \frac{\partial z_l}{\partial A_l} \\ \\
\frac{\partial C}{\partial b_l} &amp;= \textcolor{blue}{\frac{\partial C}{\partial w_L}} \textcolor{red}{\frac{\partial w_L}{\partial z_L}} \textcolor{violet}{\frac{\partial z_L}{\partial w_{L-1}}} \ldots \frac{\partial z_l}{\partial b_l}
\end{aligned}\]

<p>Actual implementations of these calculations can be found in the code below.</p>

<h3 id="stochastic-gradient-descent">Stochastic Gradient Descent</h3>
<p>Once we calculate how each of the numbers in our weights and biases impact the ultimate cost function, we need to decide how to update them such that the overall cost will decrease next time. For this, we use gradient descent. You may recall that the gradient points in the positive direction of maximum slope i.e. the gradient tells us, for a particular point in the space, which direction will a given change have the maximum impact. Since we are trying to minimize the cost function, we want to move in the <em>negative</em> direction of the same gradient vector: hence, gradient <em>descent</em>.</p>

<p>Since the gradient of the cost function is defined for the entire sample of \(N\) observations, in theory, we should have to do a forward pass of each data point in order to calculate the “true” loss. For large datasets, which we need for this type of machine learning, this becomes infeasible as you would need thousands and thousands of passes through the dataset.</p>

<p>Stochastic gradient descent is a heuristic solutions to this problem. Rather than calcualte the gradient of the cost function over the entire (training) dataset, we sample from our data and calculate the gradient over that subsample. This subsample can be as small as a single observation, and often isn’t more than 32 rows of data (i.e. observations).</p>

<p>Once we have our gradient, we update the weights according to some rule. The simplest takes the form:</p>

\[%katex
u_{t+1} = u_t - \eta \nabla C(u_t)\]

<p>Where \(u_{t+1}\) is the updated weight or bias, \(\eta\) acts as a scale for the update and is referred to as the step size or learning rate, and \(\nabla C(u_t)\) is the (stochastic) gradient of the cost function evaluated at the original point \(u_t\). To solidify this concept, we can imagine the gradient of the cost function looks something like:</p>

\[%katex
\nabla C(A_L, b_L, ..., A_1, b_1) = \begin{bmatrix}
\frac{\partial C}{\partial A_L} \\ \\
\frac{\partial C}{\partial b_L} \\
\vdots \\
\frac{\partial C}{\partial A_1} \\ \\
\frac{\partial C}{\partial b_1} \\
\end{bmatrix}\]

<p>Recalling, that the partial derivatives are calculated as above for each of the elements in the weight matricies \(A_l\) and bias vectors \(b_l\) for \(l \in [1, L]\). This is to say that, practically, we are calculating these elements backward from the last layer to the first layer</p>

\[%katex
\begin{aligned}
    a_{L(t+1)} &amp;= a_{Lt} - \eta \frac{\partial C}{\partial A_L} \\ \\
    b_{L(t+1)} &amp;= b_{Lt} - \eta \frac{\partial C}{\partial b_L} \\ 
    &amp;\,\,\, \vdots \\
    a_{1(t+1)} &amp;= a_{1t} - \eta \frac{\partial C}{\partial A_1} \\ \\
    b_{1(t+1)} &amp;= b_{1t} - \eta \frac{\partial C}{\partial b_1} \\ 
\end{aligned}\]

<h2 id="implementation">Implementation</h2>
<p>We can implement the above with the Julia programming language. Ultimately, our goal is to call a neural network with the following interface:</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">net</span> <span class="o">=</span> <span class="n">Network</span><span class="x">(</span><span class="n">inputdim</span><span class="o">=</span><span class="n">input_size</span><span class="x">,</span> <span class="n">cost</span><span class="o">=</span><span class="n">loss_mse</span><span class="x">,</span> <span class="n">dcost</span><span class="o">=</span><span class="n">dloss_mse</span><span class="x">)</span>
<span class="n">addlayer!</span><span class="x">(</span><span class="n">net</span><span class="x">,</span> <span class="mi">64</span><span class="x">,</span> <span class="n">relu</span><span class="x">,</span> <span class="n">drelu</span><span class="x">)</span> 
<span class="n">addlayer!</span><span class="x">(</span><span class="n">net</span><span class="x">,</span> <span class="mi">32</span><span class="x">,</span> <span class="n">relu</span><span class="x">,</span> <span class="n">drelu</span><span class="x">)</span> 
<span class="n">addlayer!</span><span class="x">(</span><span class="n">net</span><span class="x">,</span> <span class="n">output_size</span><span class="x">,</span> <span class="n">softmax</span><span class="x">,</span> <span class="n">dsoftmax</span><span class="x">)</span> 

<span class="c"># fit network - stochastic gradient descent </span>
<span class="n">fit!</span><span class="x">(</span><span class="n">net</span><span class="x">,</span> <span class="n">X_train</span><span class="x">,</span> <span class="n">Y_train</span><span class="x">,</span> <span class="n">batchsize</span><span class="o">=</span><span class="mi">16</span><span class="x">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="x">,</span> <span class="n">learningrate</span><span class="o">=</span><span class="mf">0.01</span><span class="x">)</span>
</code></pre></div></div>

<p>Note, we will begin by defining the neural network with an input dimmension i.e. the length of each input vector. Subsequently, we add layers to the network by defining the size of the layer’s output, the activation function, and the derivative of the activation function<sup id="fnref:6" role="doc-noteref"><a href="#fn:6" class="footnote">6</a></sup>.</p>

<h3 id="network-1">Network</h3>
<p>First, we define a new “struct” in Julia, called <code class="language-plaintext highlighter-rouge">Network</code>:</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">mutable struct</span><span class="nc"> Network</span> 
    <span class="n">inputdim</span><span class="o">::</span><span class="kt">Int16</span>
    <span class="n">layers</span><span class="o">::</span><span class="kt">Vector</span><span class="x">{</span><span class="n">Layer</span><span class="x">}</span>
    <span class="n">cost</span><span class="o">::</span><span class="kt">Function</span>
    <span class="n">dcost</span><span class="o">::</span><span class="kt">Function</span>

    <span class="c"># constructor</span>
    <span class="k">function</span><span class="nf"> Network</span><span class="x">(;</span><span class="n">inputdim</span><span class="x">,</span> <span class="n">cost</span><span class="x">,</span> <span class="n">dcost</span><span class="x">)</span>  
        <span class="n">new</span><span class="x">(</span><span class="n">inputdim</span><span class="x">,</span> <span class="kt">Array</span><span class="x">{</span><span class="n">Layer</span><span class="x">}[],</span> <span class="n">cost</span><span class="x">,</span> <span class="n">dcost</span><span class="x">)</span>
    <span class="k">end</span>
<span class="k">end</span>
</code></pre></div></div>

<p><strong>Julia notes:</strong></p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">mutable struct ...</code>. If you’re unfamiliar with a “struct”, it’s a rather simple, but powerful, concept that shows up in other programming languages like C/C++ and Go. The idea is to group variables together in a way that is meaningful to the programmer or reader. With this, we can now create one or many new <code class="language-plaintext highlighter-rouge">Network</code>s and access the components with “dot” notation e.g. <code class="language-plaintext highlighter-rouge">net.layers</code>. The <code class="language-plaintext highlighter-rouge">mutable</code> keywork just allows us to modify the elements of the struct throughtout the program - by default, structs in Julia are immutable, and values don’t change after they are first created.</li>
  <li><code class="language-plaintext highlighter-rouge">var::Type</code>. Julia allows for “type annotation”, which we’ll use define each variable a bit more precicely. In this case, the advantage is mostly for readability, and so that we know as best as possible what is happening each step of the way in our netural network. For example, when we see <code class="language-plaintext highlighter-rouge">inputdim::Int16</code>, it is simply saying that our network has a variable called <code class="language-plaintext highlighter-rouge">inputdim</code> that is a 16-bit integer. Similarly, <code class="language-plaintext highlighter-rouge">cost::Function</code> tells us (and the complier) to expect a function for the variable cost.</li>
  <li><code class="language-plaintext highlighter-rouge">function Network(;inputdim, cost, dcost)</code>. This is our “constructor”. It tells the compiler (and the programmer) how to create a new <code class="language-plaintext highlighter-rouge">Network</code>. For example, despite our <code class="language-plaintext highlighter-rouge">Network</code> having four internal variables, we create a new network with only three input arguments (because we don’t have any layers yet). One final note - the semicolon “;” before <code class="language-plaintext highlighter-rouge">inputdim</code> forces us to explicitly declare the variable we’re setting, rather than rely on the position. So we can create a new network with <code class="language-plaintext highlighter-rouge">net = Network(inputdim=4, cost=mse, dcost=dmse)</code>, but if we try to say <code class="language-plaintext highlighter-rouge">net = Network(4, mse, dmse)</code> the compiler will throw an error. This is a design choice by me, feel free to remove it for your use.</li>
</ul>

<h3 id="layer">Layer</h3>
<p>Next, we define a new struct for each Layer:</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">mutable struct</span><span class="nc"> Layer</span>
    <span class="n">weights</span><span class="o">::</span><span class="kt">AbstractArray</span>
    <span class="n">bias</span><span class="o">::</span><span class="kt">AbstractArray</span>

    <span class="c"># each layer can have its own activation function</span>
    <span class="n">activation</span><span class="o">::</span><span class="kt">Function</span>
    <span class="n">dactivation</span><span class="o">::</span><span class="kt">Function</span>

    <span class="c"># cache </span>
    <span class="c"># 1- the last linear transformation: z = Ax + b</span>
    <span class="c"># 2- activated output: w = activation(z) </span>
    <span class="n">linear_out</span><span class="o">::</span><span class="kt">AbstractArray</span>
    <span class="n">activated_out</span><span class="o">::</span><span class="kt">AbstractArray</span>

    <span class="c"># keep track of partial derivative error for each batch </span>
    <span class="n">dC_dlinear</span><span class="o">::</span><span class="kt">AbstractArray</span> 
    <span class="n">dC_dweights</span><span class="o">::</span><span class="kt">AbstractArray</span> 
    <span class="n">dC_dbias</span><span class="o">::</span><span class="kt">AbstractArray</span> 

    <span class="c"># constructor</span>
    <span class="k">function</span><span class="nf"> Layer</span><span class="x">(</span>
        <span class="n">insize</span><span class="o">::</span><span class="kt">Number</span><span class="x">,</span> 
        <span class="n">outsize</span><span class="o">::</span><span class="kt">Number</span><span class="x">,</span> 
        <span class="n">activation</span><span class="o">::</span><span class="kt">Function</span><span class="x">,</span> 
        <span class="n">dactivation</span><span class="o">::</span><span class="kt">Function</span><span class="x">)</span>

        <span class="c"># the Glorot normal initializer, also called Xavier normal initializer</span>
        <span class="c">#</span>
        <span class="c"># reference: </span>
        <span class="c"># https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/initializers/initializers_v2.py</span>
        <span class="n">sigma_sq</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">/</span> <span class="x">(</span><span class="n">insize</span> <span class="o">+</span> <span class="n">outsize</span><span class="x">)</span>
        <span class="n">weights</span> <span class="o">=</span> <span class="n">randn</span><span class="x">(</span><span class="n">outsize</span><span class="x">,</span> <span class="n">insize</span><span class="x">)</span> <span class="o">*</span> <span class="n">sigma_sq</span>
        <span class="n">bias</span> <span class="o">=</span> <span class="n">randn</span><span class="x">(</span><span class="n">outsize</span><span class="x">,</span> <span class="mi">1</span><span class="x">)</span> <span class="o">*</span> <span class="n">sigma_sq</span>

        <span class="n">linear_out</span> <span class="o">=</span> <span class="n">zeros</span><span class="x">(</span><span class="n">outsize</span><span class="x">,</span> <span class="mi">1</span><span class="x">)</span>
        <span class="n">activated_out</span> <span class="o">=</span> <span class="n">zeros</span><span class="x">(</span><span class="n">outsize</span><span class="x">,</span> <span class="mi">1</span><span class="x">)</span>

        <span class="n">dC_dlinear</span> <span class="o">=</span> <span class="n">zeros</span><span class="x">(</span><span class="n">outsize</span><span class="x">,</span> <span class="mi">1</span><span class="x">)</span>
        <span class="n">dC_dweights</span> <span class="o">=</span> <span class="n">zeros</span><span class="x">(</span><span class="n">outsize</span><span class="x">,</span> <span class="n">insize</span><span class="x">)</span>
        <span class="n">dC_dbias</span> <span class="o">=</span> <span class="n">zeros</span><span class="x">(</span><span class="n">outsize</span><span class="x">,</span> <span class="mi">1</span><span class="x">)</span>
        <span class="n">new</span><span class="x">(</span><span class="n">weights</span><span class="x">,</span> <span class="n">bias</span><span class="x">,</span> <span class="n">activation</span><span class="x">,</span> <span class="n">dactivation</span><span class="x">,</span> <span class="n">linear_out</span><span class="x">,</span> <span class="n">activated_out</span><span class="x">,</span> <span class="n">dC_dlinear</span><span class="x">,</span> <span class="n">dC_dweights</span><span class="x">,</span> <span class="n">dC_dbias</span><span class="x">)</span>
    <span class="k">end</span>
<span class="k">end</span>
</code></pre></div></div>

<p><strong>Julia note:</strong></p>
<ul>
  <li>Similar to the <code class="language-plaintext highlighter-rouge">Network</code> above, we create a new mutable struct to represent a <code class="language-plaintext highlighter-rouge">Layer</code>. One practical note is that we use <code class="language-plaintext highlighter-rouge">AbstractArray</code> because it allows us to set those variables as either a one-dimensional vector, or two-dimensional matrix. This works because of type inheritance, which is worth reading about in the official Julia documentation.</li>
</ul>

<p><strong>Network note:</strong></p>
<ul>
  <li>Each layer will store information about its current weights and biases, as well as its activation function and the derivative.</li>
  <li>Additionally, each layer will store some values for the backpropagation step. Namely, we will keep track of the last output for both the linear and “activated” vectors, as well as the running total of the various relevant partial derivatives i.e. the partial derivative of the cost (loss) function with respect to the linear transformation \(z\), the weights \(A\), and the bias vector \(b\).</li>
</ul>

<h4 id="initializing-the-layer">Initializing the Layer</h4>
<p>When we create a new layer, we need to make a decision about how to initialize the weights and bias. Zeros are not a good choice, because our first forward passes will be uninteresting (the result is the same for all input), and it will be difficult to update during backpropagation. Thus, we want to initialize the weights and bias with some random component, the specifics of which we follow the default setting in Google’s Tensorflow and use a so-called Glorot normal. This process basically allows us to initialize the weights and bias from a normal distribution with mean zero, and a variance that depends on the size of the inputs and outputs.</p>

<p>Besides that, we set all of the “cache” values equal to zeros, since they will be calculated in due time.</p>

<h3 id="feedforward">Feedforward</h3>
<p>For each step, we need to make a forward pass of the network, which is a fancy way of saying we need to take an input \(x\) and transform it to an output \(w_L\).</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">function</span><span class="nf"> feedforward!</span><span class="x">(</span><span class="n">net</span><span class="o">::</span><span class="n">Network</span><span class="x">,</span> <span class="n">input</span><span class="o">::</span><span class="kt">AbstractArray</span><span class="x">)</span><span class="o">::</span><span class="kt">AbstractArray</span>
    <span class="n">nlayers</span> <span class="o">=</span> <span class="n">length</span><span class="x">(</span><span class="n">net</span><span class="o">.</span><span class="n">layers</span><span class="x">)</span>

    <span class="n">lastoutput</span> <span class="o">=</span> <span class="n">input</span> 
    <span class="k">for</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">1</span><span class="o">:</span><span class="n">nlayers</span>
        <span class="n">layer</span> <span class="o">=</span> <span class="n">net</span><span class="o">.</span><span class="n">layers</span><span class="x">[</span><span class="n">i</span><span class="x">]</span>

        <span class="c"># linear_out: z = Ax + b </span>
        <span class="n">layer</span><span class="o">.</span><span class="n">linear_out</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">weights</span> <span class="o">*</span> <span class="n">lastoutput</span> <span class="o">+</span> <span class="n">layer</span><span class="o">.</span><span class="n">bias</span>

        <span class="c"># activated_output: w = activation(Ax + b)</span>
        <span class="n">layer</span><span class="o">.</span><span class="n">activated_out</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">activation</span><span class="x">(</span><span class="n">layer</span><span class="o">.</span><span class="n">linear_out</span><span class="x">)</span>

        <span class="c"># update for the next input</span>
        <span class="n">lastoutput</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">activated_out</span> 
    <span class="k">end</span>

    <span class="c"># w_L</span>
    <span class="k">return</span> <span class="n">lastoutput</span>
<span class="k">end</span>
</code></pre></div></div>

<p>This code should be pretty simple: starting with the input, for each layer in the network, calculate the linear transformation \(z = Ax + b\), the activated transformation \(w = \sigma(z)\), and then use the previous layer’s output as input to the next layer.</p>

<p><strong>Julia note:</strong></p>
<ul>
  <li>By convention, when a function can modify the element it’s being passed, we include an exclaimation mark “!” at the end of the function i.e. <code class="language-plaintext highlighter-rouge">feedforward!</code> as opposed to <code class="language-plaintext highlighter-rouge">feedforward</code>. This is related to a paradigm called “pass by reference” in which you allow the function to modify the actual object you passed it. In contrast, “pass by value” will make a copy of the object’s value, do some operation, and return a new value. Both are useful, and Julia allows for both. In this case, we want these functions to directly update the actual network.</li>
  <li>Additionally, just like type annotation with variables, in Julia we can give the compiler (and the programmer) a hint that this function returns a value of type <code class="language-plaintext highlighter-rouge">AbstractArray</code>. We signify that by <code class="language-plaintext highlighter-rouge">fctname()::ReturnType</code>.</li>
</ul>

<h3 id="backpropagation-1">Backpropagation</h3>
<p>Based on our understanding with the big picture above, backpropagation is perhaps more aptly named “calculate partial derivatives”. One key note is that we will keep a running total of the partial derivatives for each input until we are ready to actually use them to update the weights and bias. The batch size of our stochastic gradient descent will tell us how many inputs the network will “see” before updating the weights.</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">function</span><span class="nf"> backpropagate!</span><span class="x">(</span><span class="n">net</span><span class="o">::</span><span class="n">Network</span><span class="x">,</span> <span class="n">x</span><span class="o">::</span><span class="kt">AbstractArray</span><span class="x">,</span> <span class="n">truth</span><span class="o">::</span><span class="kt">AbstractArray</span><span class="x">)</span>

    <span class="c"># calculate last layer partials </span>
    <span class="n">lastlayer</span> <span class="o">=</span> <span class="n">net</span><span class="o">.</span><span class="n">layers</span><span class="x">[</span><span class="k">end</span><span class="x">]</span>

    <span class="n">lastlayer</span><span class="o">.</span><span class="n">dC_dlinear</span>   <span class="o">=</span> <span class="n">lastlayer</span><span class="o">.</span><span class="n">dactivation</span><span class="x">(</span><span class="n">lastlayer</span><span class="o">.</span><span class="n">linear_out</span><span class="x">)</span> <span class="o">*</span> <span class="n">net</span><span class="o">.</span><span class="n">dcost</span><span class="x">(</span><span class="n">lastlayer</span><span class="o">.</span><span class="n">activated_out</span><span class="x">,</span> <span class="n">truth</span><span class="x">)</span>
    <span class="n">lastlayer</span><span class="o">.</span><span class="n">dC_dweights</span> <span class="o">+=</span> <span class="n">lastlayer</span><span class="o">.</span><span class="n">dC_dlinear</span> <span class="o">*</span> <span class="n">net</span><span class="o">.</span><span class="n">layers</span><span class="x">[</span><span class="k">end</span> <span class="o">-</span> <span class="mi">1</span><span class="x">]</span><span class="o">.</span><span class="n">activated_out</span><span class="err">'</span>
    <span class="n">lastlayer</span><span class="o">.</span><span class="n">dC_dbias</span>    <span class="o">+=</span> <span class="n">lastlayer</span><span class="o">.</span><span class="n">dC_dlinear</span>

    <span class="c"># iterate through previous layer partials</span>
    <span class="c"># note arrays are indexed at 1, not 0</span>
    <span class="k">for</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">1</span><span class="o">:</span><span class="x">(</span><span class="n">length</span><span class="x">(</span><span class="n">net</span><span class="o">.</span><span class="n">layers</span><span class="x">)</span> <span class="o">-</span> <span class="mi">1</span><span class="x">)</span>
        <span class="n">layer</span>     <span class="o">=</span> <span class="n">net</span><span class="o">.</span><span class="n">layers</span><span class="x">[</span><span class="k">end</span> <span class="o">-</span> <span class="n">i</span><span class="x">]</span>      <span class="c"># layer "l"</span>
        <span class="n">nextlayer</span> <span class="o">=</span> <span class="n">net</span><span class="o">.</span><span class="n">layers</span><span class="x">[</span><span class="k">end</span> <span class="o">-</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="x">]</span>  <span class="c"># nextlayer "l + 1"</span>

        <span class="c"># select the output of the previous layer</span>
        <span class="c"># note, for the first layer this will be the original input, xi</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">&lt;</span> <span class="n">length</span><span class="x">(</span><span class="n">net</span><span class="o">.</span><span class="n">layers</span><span class="x">)</span>
            <span class="n">prevlayer</span> <span class="o">=</span> <span class="n">net</span><span class="o">.</span><span class="n">layers</span><span class="x">[</span><span class="k">end</span> <span class="o">-</span> <span class="n">i</span> <span class="o">-</span> <span class="mi">1</span><span class="x">]</span>
            <span class="n">prevout</span>   <span class="o">=</span> <span class="n">prevlayer</span><span class="o">.</span><span class="n">activated_out</span>

        <span class="k">elseif</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">==</span> <span class="n">length</span><span class="x">(</span><span class="n">net</span><span class="o">.</span><span class="n">layers</span><span class="x">)</span>
            <span class="n">prevout</span> <span class="o">=</span> <span class="n">x</span>
        <span class="k">end</span>

        <span class="n">layer</span><span class="o">.</span><span class="n">dC_dlinear</span>   <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">dactivation</span><span class="x">(</span><span class="n">layer</span><span class="o">.</span><span class="n">linear_out</span><span class="x">)</span> <span class="o">*</span> <span class="n">nextlayer</span><span class="o">.</span><span class="n">weights</span><span class="err">'</span> <span class="o">*</span> <span class="n">nextlayer</span><span class="o">.</span><span class="n">dC_dlinear</span> 
        <span class="n">layer</span><span class="o">.</span><span class="n">dC_dweights</span> <span class="o">+=</span> <span class="n">layer</span><span class="o">.</span><span class="n">dC_dlinear</span> <span class="o">*</span> <span class="n">prevout</span><span class="err">'</span>
        <span class="n">layer</span><span class="o">.</span><span class="n">dC_dbias</span>    <span class="o">+=</span> <span class="n">layer</span><span class="o">.</span><span class="n">dC_dlinear</span> 
    <span class="k">end</span>
<span class="k">end</span>
</code></pre></div></div>

<h3 id="stochastic-gradient-descent-1">Stochastic Gradient Descent</h3>
<p>Putting it all together, our stochastic gradient descent step breaks the input data into separate batches, and performs the forward pass, the backpropagation, and updates the weights according to the batch size. Together, we see:</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">function</span><span class="nf"> sgd!</span><span class="x">(</span>
    <span class="n">net</span><span class="o">::</span><span class="n">Network</span><span class="x">,</span> 
    <span class="n">x</span><span class="o">::</span><span class="kt">AbstractArray</span><span class="x">,</span> 
    <span class="n">y</span><span class="o">::</span><span class="kt">AbstractArray</span><span class="x">,</span> 
    <span class="n">batchsize</span><span class="o">::</span><span class="kt">Number</span><span class="x">,</span> 
    <span class="n">epochs</span><span class="o">::</span><span class="kt">Number</span><span class="x">,</span> 
    <span class="n">learningrate</span><span class="o">::</span><span class="kt">Number</span><span class="x">)</span>
    <span class="c"># stochastic gradient descent (sgd)</span>

    <span class="c"># input vars</span>
    <span class="n">nobs</span><span class="x">,</span> <span class="n">nvars</span> <span class="o">=</span> <span class="n">size</span><span class="x">(</span><span class="n">x</span><span class="x">)</span>

    <span class="c"># how many times do we go through the dataset? </span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="o">=</span> <span class="mi">1</span><span class="o">:</span><span class="n">epochs</span>

        <span class="c"># shuffle rows of matrix  </span>
        <span class="n">shuffledrows</span> <span class="o">=</span> <span class="n">shuffle</span><span class="x">(</span><span class="mi">1</span><span class="o">:</span><span class="n">nobs</span><span class="x">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="x">[</span><span class="n">shuffledrows</span><span class="x">,</span> <span class="o">:</span><span class="x">]</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="x">[</span><span class="n">shuffledrows</span><span class="x">,</span> <span class="o">:</span><span class="x">]</span>

        <span class="c"># track average losses for each sample in batch</span>
        <span class="n">losses</span> <span class="o">=</span> <span class="kt">Vector</span><span class="x">{</span><span class="kt">Number</span><span class="x">}();</span>

        <span class="c"># create mini batches and loop through each batch </span>
        <span class="c"># note: julia is NOT zero indexed </span>
        <span class="c">#       i.e. x[1] is the first element</span>
        <span class="k">for</span> <span class="n">batchend</span> <span class="o">=</span> <span class="n">batchsize</span><span class="o">:</span><span class="n">batchsize</span><span class="o">:</span><span class="n">nobs</span>
            <span class="n">batchstart</span> <span class="o">=</span> <span class="n">batchend</span> <span class="o">-</span> <span class="n">batchsize</span> <span class="o">+</span> <span class="mi">1</span>

            <span class="c"># get sample of rows i.e. observations </span>
            <span class="c"># and transpose into columns for feedforward</span>
            <span class="n">xbatch</span> <span class="o">=</span> <span class="n">x</span><span class="x">[</span><span class="n">batchstart</span><span class="o">:</span><span class="n">batchend</span><span class="x">,</span> <span class="o">:</span><span class="x">]</span><span class="err">'</span>
            <span class="n">ybatch</span> <span class="o">=</span> <span class="n">y</span><span class="x">[</span><span class="n">batchstart</span><span class="o">:</span><span class="n">batchend</span><span class="x">,</span> <span class="o">:</span><span class="x">]</span><span class="err">'</span>

            <span class="k">for</span> <span class="n">icol</span> <span class="k">in</span> <span class="mi">1</span><span class="o">:</span><span class="n">batchsize</span>
                <span class="n">xi</span>    <span class="o">=</span> <span class="n">xbatch</span><span class="x">[</span><span class="o">:</span><span class="x">,</span> <span class="n">icol</span><span class="o">:</span><span class="n">icol</span><span class="x">]</span>
                <span class="n">ytrue</span> <span class="o">=</span> <span class="n">ybatch</span><span class="x">[</span><span class="o">:</span><span class="x">,</span> <span class="n">icol</span><span class="o">:</span><span class="n">icol</span><span class="x">]</span>

                <span class="c"># feedforward</span>
                <span class="n">out</span> <span class="o">=</span> <span class="n">feedforward!</span><span class="x">(</span><span class="n">net</span><span class="x">,</span> <span class="n">xi</span><span class="x">)</span>

                <span class="c"># calculate loss and store for tracking progress</span>
                <span class="n">iloss</span> <span class="o">=</span> <span class="n">net</span><span class="o">.</span><span class="n">cost</span><span class="x">(</span><span class="n">out</span><span class="x">,</span> <span class="n">ytrue</span><span class="x">)</span>
                <span class="n">push!</span><span class="x">(</span><span class="n">losses</span><span class="x">,</span> <span class="n">iloss</span><span class="x">)</span>

                <span class="c"># calculate partial derivatives of each weight and bias</span>
                <span class="c"># i.e. backpropagate</span>
                <span class="n">backpropagate!</span><span class="x">(</span><span class="n">net</span><span class="x">,</span> <span class="n">xi</span><span class="x">,</span> <span class="n">ytrue</span><span class="x">)</span>
            <span class="k">end</span>

            <span class="c"># update weights and bias </span>
            <span class="n">update!</span><span class="x">(</span><span class="n">net</span><span class="x">,</span> <span class="n">learningrate</span><span class="x">)</span>
        <span class="k">end</span>

        <span class="c"># sample average loss from batch to track progress</span>
        <span class="n">push!</span><span class="x">(</span><span class="n">epochlosses</span><span class="x">,</span> <span class="n">mean</span><span class="x">(</span><span class="n">losses</span><span class="x">))</span>
    <span class="k">end</span>
<span class="k">end</span>
</code></pre></div></div>
<p>And note that each update step on the network adjusts the weights and biases as follows:</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">function</span><span class="nf"> update!</span><span class="x">(</span><span class="n">net</span><span class="o">::</span><span class="n">Network</span><span class="x">,</span> <span class="n">learningrate</span><span class="o">::</span><span class="kt">Number</span><span class="x">)</span>

    <span class="c"># update weights in each layer based on the error terms dC_dweights, dC_dbias</span>
    <span class="k">for</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">1</span><span class="o">:</span><span class="n">length</span><span class="x">(</span><span class="n">net</span><span class="o">.</span><span class="n">layers</span><span class="x">)</span>
        <span class="n">layer</span> <span class="o">=</span> <span class="n">net</span><span class="o">.</span><span class="n">layers</span><span class="x">[</span><span class="n">i</span><span class="x">]</span>

        <span class="c"># gradient descent step </span>
        <span class="n">layer</span><span class="o">.</span><span class="n">weights</span> <span class="o">-=</span> <span class="n">learningrate</span> <span class="o">*</span> <span class="n">layer</span><span class="o">.</span><span class="n">dC_dweights</span>
        <span class="n">layer</span><span class="o">.</span><span class="n">bias</span>    <span class="o">-=</span> <span class="n">learningrate</span> <span class="o">*</span> <span class="n">layer</span><span class="o">.</span><span class="n">dC_dbias</span>

        <span class="c"># reset the error terms for next batch </span>
        <span class="n">nrows</span><span class="x">,</span> <span class="n">ncols</span> <span class="o">=</span> <span class="n">size</span><span class="x">(</span><span class="n">layer</span><span class="o">.</span><span class="n">weights</span><span class="x">)</span>

        <span class="n">layer</span><span class="o">.</span><span class="n">dC_dlinear</span>  <span class="o">=</span> <span class="n">zeros</span><span class="x">(</span><span class="n">nrows</span><span class="x">,</span> <span class="mi">1</span><span class="x">)</span>
        <span class="n">layer</span><span class="o">.</span><span class="n">dC_dweights</span> <span class="o">=</span> <span class="n">zeros</span><span class="x">(</span><span class="n">nrows</span><span class="x">,</span> <span class="n">ncols</span><span class="x">)</span>
        <span class="n">layer</span><span class="o">.</span><span class="n">dC_dbias</span>    <span class="o">=</span> <span class="n">zeros</span><span class="x">(</span><span class="n">nrows</span><span class="x">,</span> <span class="mi">1</span><span class="x">)</span>
    <span class="k">end</span>
<span class="k">end</span>
</code></pre></div></div>

<h2 id="conclusion">Conclusion</h2>
<p>That concludes the basics. The full code is available here:</p>
<ul>
  <li><a href="https://github.com/slee981/neural-network">https://github.com/slee981/neural-network</a>.</li>
</ul>

<p>For me, the key takeaways from this exercise are:</p>
<ol>
  <li>Neural networks are intricate combinations of math, but not magic.</li>
  <li>The complex layering of weights, biases, and activation functions makes interpretation very challenging, if not useless. This is to suggest that neural networks are fantastic at fitting data, but that reverse engineering <em>why</em> or <em>how</em> the neural network arrives at a given answer is beyond the scope of our current tool kit.</li>
</ol>

<p>The implications of this are to consider when a neural network is the right tool for the job, and when it might not be.</p>

<p>For example, I suspect many people would be uneasy with the Federal Reserve setting the interest rate based on predictions from a neural network. Rather, Reserve Bank presidents and their teams of researchers have an (implicit or explicit) responsibilty to <em>explain</em> to congress, and the American people, why they made their decision. Much effort goes into these written and verbal explainations, and much effort occurs behind the scenes to estimate the implications of their decisions through the lens of cause and effect.</p>

<p>Alternatively, a neural network’s tremendous power at fitting patterns in data is perfectly geared to other problems, for example, image recognition for a search engine, or foreign language translation. In these cases, there is much less emphasis on <em>why</em> and much more emphasis on <em>what</em> or <em>which</em>.</p>

<p>Thus, these different questions we ask ourselves may require different tools: in some cases, prediction alone may not be all that we are after.</p>

<h2 id="resources">Resources</h2>
<ol>
  <li>Learning From Data, Gilbert Strang: <a href="http://math.mit.edu/~gs/learningfromdata/">http://math.mit.edu/~gs/learningfromdata/</a></li>
  <li>Michael Nielsen’s “Neural Networks and Deep Learning”: <a href="http://neuralnetworksanddeeplearning.com/">http://neuralnetworksanddeeplearning.com/</a></li>
  <li>Blog post on computational graphs: <a href="https://colah.github.io/posts/2015-08-Backprop/">https://colah.github.io/posts/2015-08-Backprop/</a>.</li>
</ol>

<h2 id="footnotes">Footnotes</h2>
<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>In Keras, this is what you get when you setup a <code class="language-plaintext highlighter-rouge">Sequential</code> model with <code class="language-plaintext highlighter-rouge">Dense</code> layers. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p>I emphasize feasible because the stochastic gradient descent is an optimization technique that approximates the gradient of the loss function over some subsample of the data. However, without this (or some) optimization, training the network would simply take too long to be useful. . <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:3" role="doc-endnote">
      <p>Bias is a rather unfortunate naming convention for this vector. It might be more convenient to think of it as the intercept in the standard equation for a line \(y = ax + b\). This is, it should not really be associated with bias as we typically think of it i.e. a tendency to error in a particular way, but rather as a standard component of an affine transformation. <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:4" role="doc-endnote">
      <p>For an excellent introduction to deravitives in computational graphs, see this link: https://colah.github.io/posts/2015-08-Backprop/. <a href="#fnref:4" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:5" role="doc-endnote">
      <p>See code for various examples of how to “reduce” a vector of losses into a single number: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/losses/losses_impl.py. <a href="#fnref:5" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:6" role="doc-endnote">
      <p>Frameworks like Flux.jl and Tensorflow implement some type of autograd system that can handle the differentiation step without explictly passing the derivative function. For the sake of this tutorial, explicitly writing out a few derivatives is both helpful and simpler. <a href="#fnref:6" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>

    </div>
  </div>

  <div class="d-flex justify-content-center pt-5">
    <h2>More</h2>
  </div>

  <div class="row pt-5">
    <div class="col-sm-6">
      <div class="card info-card sticky-div h-100">

  <div class="card-body">

    <h3 class="card-title text-center">Recent Posts</h3>
  </div>
    <ul class="list-group list-group-flush">
      
        <li class="list-group-item">
          <time datetime="2020-12-20T00:00:00-06:00">Jul 14, 2021</time> 
          &raquo;
          <a class="card-link" href="/networks/2021/07/14/networks-notation.html">Networks (Part I) - Notation</a>
        </li>
      
        <li class="list-group-item">
          <time datetime="2020-12-20T00:00:00-06:00">Jul 13, 2021</time> 
          &raquo;
          <a class="card-link" href="/potential%20outcomes/2021/07/13/what-is-potential-outcomes.html">What Is Potential Outcomes</a>
        </li>
      
        <li class="list-group-item">
          <time datetime="2020-12-20T00:00:00-06:00">Jul 12, 2021</time> 
          &raquo;
          <a class="card-link" href="/instrumental%20variables/2021/07/12/sem-iv.html">SEM Estimation by 2SLS</a>
        </li>
      
        <li class="list-group-item">
          <time datetime="2020-12-20T00:00:00-06:00">Jan 21, 2021</time> 
          &raquo;
          <a class="card-link" href="/econometrics/statistics/2021/01/21/why-omitted-variables-matter.html">Omitted Variable Bias by Simulation</a>
        </li>
      
        <li class="list-group-item">
          <time datetime="2020-12-20T00:00:00-06:00">Jan 19, 2021</time> 
          &raquo;
          <a class="card-link" href="/statistics/tutorial/2021/01/19/moving-average-in-R.html">Moving Average in R, tidyverse</a>
        </li>
      
    </ul>
  <div class="card-body">
  </div>
</div>

    </div>
    <div class="col-sm-6">
        <div class="card info-card sticky-div h-100">

    <div class="card-body">

      <h3 class="card-title text-center">Categories</h3>

      
        <div class="category-tags d-flex flex-wrap justify-content-center">
          
            
            
            
            
            <a class="card-link my-2 py-2 px-3" href="/categories.html#general">
              
              
                General
              
            
            </a>
          
            
            
            
            
            <a class="card-link my-2 py-2 px-3" href="/categories.html#tutorial">
              
              
                Tutorial
              
            
            </a>
          
            
            
            
            
            <a class="card-link my-2 py-2 px-3" href="/categories.html#econometrics">
              
              
                Econometrics
              
            
            </a>
          
            
            
            
            
            <a class="card-link my-2 py-2 px-3" href="/categories.html#machine-learning">
              
              
                Machine
              
                Learning
              
            
            </a>
          
            
            
            
            
            <a class="card-link my-2 py-2 px-3" href="/categories.html#prediction">
              
              
                Prediction
              
            
            </a>
          
            
            
            
            
            <a class="card-link my-2 py-2 px-3" href="/categories.html#statistics">
              
              
                Statistics
              
            
            </a>
          
            
            
            
            
            <a class="card-link my-2 py-2 px-3" href="/categories.html#instrumental-variables">
              
              
                Instrumental
              
                Variables
              
            
            </a>
          
            
            
            
            
            <a class="card-link my-2 py-2 px-3" href="/categories.html#potential-outcomes">
              
              
                Potential
              
                Outcomes
              
            
            </a>
          
            
            
            
            
            <a class="card-link my-2 py-2 px-3" href="/categories.html#networks">
              
              
                Networks
              
            
            </a>
          
        </div>
      
    </div>
  </div>
    </div>
  </div>

  
    <div class="row">
      <div class="row mt-4" id="disqus_thread"></div>
<script>
  var disqus_config = function () {
    this.page.url = 'https://sparklingcorrelation.com/machine%20learning/prediction/tutorial/2020/12/20/build-a-neural-network.html';
    this.page.identifier = 'https://sparklingcorrelation.com/machine%20learning/prediction/tutorial/2020/12/20/build-a-neural-network.html';
  };

  (function() {
    var d = document, s = d.createElement('script');

    s.src = 'https://sparklingcorrelation.disqus.com/embed.js';

    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
  })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>

    </div>
  
      
</div>
      </main>

    </div>
    <footer class="text-muted">
  <div class="container">
    <div class="row">
      <div class="d-flex justify-content-center">
        <div class="footer-element">
        <p>&copy;2021 Sparkling Correlation</p>
    
</div>
      </div>
    </div>

    
      <div class="row">
        <div class="d-flex justify-content-center">
          <div class="footer-element">
    
        <a class="social-icon" href="mailto:smlee.981@gmail.com" target="_blank">
            <i class="fa-hover fas fa-envelope" title="Email"></i>
        </a>
    
        <a class="social-icon" href="https://twitter.com/slee981" target="_blank">
            <i class="fa-hover fab fa-twitter" title="Twitter"></i>
        </a>
    
        <a class="social-icon" href="https://github.com/slee981" target="_blank">
            <i class="fa-hover fab fa-github" title="GitHub"></i>
        </a>
    
</div>

        </div>
      </div>
    

    
      <div class="row">
        <div class="d-flex justify-content-center">
           <p class="powered-by footer-element mt-3">
    Powered by 
    <a href="https://jekyllrb.com/" target="_blank">Jekyll</a> & 
    <a href="https://github.com/slee981/jekyll-theme-cadre" target="_blank">Cadre</a>
</p>

        </div>
      </div>
    
  </div>
</footer>



    
    
    
      <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css"
  integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X"
  crossorigin="anonymous">

<!-- The loading of KaTeX is deferred to speed up page rendering -->
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js"
  integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4"
  crossorigin="anonymous"></script>

<!-- To automatically render math in text elements, include the auto-render extension: -->
<script defer
  src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js"
  integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa"
  crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>

    
    
    
      <script async src="https://www.googletagmanager.com/gtag/js?id=UA-138925131-2"></script>
<script>
  window['ga-disable-UA-138925131-2'] = window.doNotTrack === "1" || navigator.doNotTrack === "1" || navigator.doNotTrack === "yes" || navigator.msDoNotTrack === "1";
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-138925131-2');
</script>

    
  </body>
</html>
